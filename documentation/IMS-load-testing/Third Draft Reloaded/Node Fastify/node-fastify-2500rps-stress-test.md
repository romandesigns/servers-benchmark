
# ðŸ“Š Benchmark Results: Node Fastify â€“ 2500 RPS Stress Test

## ðŸ§ª Consolidated Performance Table

| Round | CPU / RAM   | Total Requests | Avg Resp Time | p95 Resp Time | Max Resp Time | RPS     | Error % | CPU Max | Mem Max  |
|-------|-------------|---------------|---------------|---------------|---------------|---------|---------|---------|----------|
| 1     | 1 CPU / 1GB | 338,030       | 1.55 s        | 2.27 s        | 11.23 s       | 1,819/s | 0%      | ~100%   | ~129 MB  |
| 2     | 2 CPU / 2GB | 338,295       | 1.55 s        | 2.27 s        | 11.23 s       | 1,819/s | 0%      | ~100%   | ~129 MB  |
| 3     | 4 CPU / 4GB | 338,705       | 1.55 s        | 2.27 s        | 11.23 s       | 1,819/s | 0%      | ~100%   | ~129 MB  |

---

## ðŸ§  Observations & Notes

### âš  Round 1 (1 CPU / 1GB)
- CPU saturated at **~100%** for the entire test â€” **clear single-thread bottleneck**.
- Memory usage peaked at **~129 MB**, well within limits.
- Sustained **~1,819 RPS** for 3 minutes.
- Latencies: **Avg**: 1.55 s, **p95**: 2.27 s, **Max**: 11.23 s.
- Network I/O: ~129 MB received, ~56 MB sent.

### âš  Round 2 (2 CPU / 2GB)
- CPU still pinned at **~100%** â€” extra cores did not improve throughput due to single-threaded nature.
- Memory remained at ~129 MB peak.
- RPS unchanged (~1,819/s), same latency profile as Round 1.
- No failed requests; request queueing caused stable but high latencies.

### âš  Round 3 (4 CPU / 4GB)
- CPU usage still locked at **~100%** on a single core.
- Memory usage identical (~129 MB).
- Throughput remained **capped at ~1,819 RPS**; extra RAM/cores provided no benefit.
- Latencies remained identical â€” server saturated but stable.

---

## ðŸ”§ Supporting Assets

| ROUNDS | K6 Stats      |
|--------|---------------|
| 1      | ![K6](sandbox:/mnt/data/bc264f60-2f6f-490a-b18a-9a5d6967c0b2.png) |
| 2      | ![K6](sandbox:/mnt/data/197beb94-4051-4e19-8a53-c0cba61220bd.png) |
| 3      | ![K6](sandbox:/mnt/data/fa17eeeb-a1a4-42a8-935d-142b0ccca03c.png) |

---

## ðŸ“‹ Final Assessment

This **meltdown scenario** confirms:

- **Throughput ceiling**: ~1,819 RPS is the hard limit under this workload for single-threaded Node Fastify.
- **Multi-core scaling is ineffective** without Node Cluster or worker processes â€” extra CPU resources are unused.
- **Latency spikes** are purely from request queuing when the single-thread loop is saturated.
- Memory consumption is trivial compared to available RAM; **CPU is the only bottleneck** here.
- **Next step**: Try the same configuration with Node Cluster or .NET multi-threaded handling to see how well they exploit extra cores under identical load.

---

## ðŸ“¦ Load Test Breakdown

### **K6 Scenario Configuration**
```javascript
export const options = {
  scenarios: {
    consistent_requests: {
      executor: "constant-arrival-rate",
      rate: 2500,            // 2,500 iterations per second
      timeUnit: "1s",        // â†’ true 2,500 RPS
      duration: "3m",
      // VUs â‰ˆ RPS Ã— avg_latency_s â†’ 2500 Ã— 1.05 â‰ˆ 2625 (worst case)
      preAllocatedVUs: 2000, // warm pool so ramp-up doesn't stall
      maxVUs: 3000,          // enough headroom for slow tiers
      gracefulStop: "0s",
    },
  },
  thresholds: {
    checks: ["rate==1.0"], // pass/fail condition
  },
  discardResponseBodies: true,
  insecureSkipTLSVerify: true,
};
```

### **Execution Details**
- **Target RPS:** 2,500 (true rate due to `timeUnit: "1s"`).
- **Duration:** 3 minutes.
- **Total Iterations per Round:** ~338k requests.
- **VUs Provisioned:** 2,000 pre-allocated, max 3,000.
- **Avg Latency Observed:** ~1.55s (heavily queue-bound under load).
- **System Under Test:** Node Fastify CRUD API (single-threaded).
- **Test Environment:** Dockerized container with varying CPU/RAM limits per round.

---

**Conclusion:** This load profile pushed the Node Fastify server to full CPU saturation on one core, revealing its single-thread performance ceiling. Further scaling requires a multi-process architecture.
